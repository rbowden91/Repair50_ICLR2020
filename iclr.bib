% Structure learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Predicting Program Properties from "Big Code"
% https://dl.acm.org/citation.cfm?doid=2676726.2677009
@inproceedings{Raychev:2015:PPP:2676726.2677009,
 author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
 title = {Predicting Program Properties from "Big Code"},
 booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
 series = {POPL '15},
 year = {2015},
 isbn = {978-1-4503-3300-9},
 location = {Mumbai, India},
 pages = {111--124},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2676726.2677009},
 doi = {10.1145/2676726.2677009},
 acmid = {2677009},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big code, closure compiler, conditional random fields, javascript, names, program properties, structured prediction, types},
}

% Structure Learning in Graphical Modeling
% https://arxiv.org/abs/1606.02359
% https://www.researchgate.net/publication/303859258_Structure_Learning_in_Graphical_Modeling
@article{article,
author = {Drton, Mathias and Maathuis, Marloes},
year = {2016},
month = {06},
pages = {},
title = {Structure Learning in Graphical Modeling},
volume = {4},
journal = {Annual Review of Statistics and Its Application},
doi = {10.1146/annurev-statistics-060116-053803}
}

% Using Markov Blankets for Causal Structure Learning
% https://dl.acm.org/citation.cfm?id=1442776
% http://www.jmlr.org/papers/volume9/pellet08a/pellet08a.pdf
@article{Pellet:2008:UMB:1390681.1442776,
 author = {Pellet, Jean-Philippe and Elisseeff, Andr{\'e}},
 title = {Using Markov Blankets for Causal Structure Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {6/1/2008},
 volume = {9},
 month = jun,
 year = {2008},
 issn = {1532-4435},
 pages = {1295--1342},
 numpages = {48},
 url = {http://dl.acm.org/citation.cfm?id=1390681.1442776},
 acmid = {1442776},
 publisher = {JMLR.org},
}

% Fast full parsing by linear-chain conditional random fields
% https://dl.acm.org/citation.cfm?id=1609155
@inproceedings{Tsuruoka:2009:FFP:1609067.1609155,
 author = {Tsuruoka, Yoshimasa and Tsujii, Jun'ichi and Ananiadou, Sophia},
 title = {Fast Full Parsing by Linear-chain Conditional Random Fields},
 booktitle = {Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics},
 series = {EACL '09},
 year = {2009},
 location = {Athens, Greece},
 pages = {790--798},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=1609067.1609155},
 acmid = {1609155},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

% Conditional Random Fields as Recurrent Neural Networks
% https://arxiv.org/abs/1502.03240
% https://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf
@article{DBLP:journals/corr/ZhengJRVSDHT15,
  author    = {Shuai Zheng and
               Sadeep Jayasumana and
               Bernardino Romera{-}Paredes and
               Vibhav Vineet and
               Zhizhong Su and
               Dalong Du and
               Chang Huang and
               Philip H. S. Torr},
  title     = {Conditional Random Fields as Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1502.03240},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03240},
  archivePrefix = {arXiv},
  eprint    = {1502.03240},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhengJRVSDHT15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}

% Program Repair
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Fair and Balanced? Bias in Bug-Fix Datasets
% https://dl.acm.org/citation.cfm?id=1595716
% We are interested in beginner program repair, which likely has a very different distribution of mistakes from expert program repair!
@inproceedings{Bird:2009:FBB:1595696.1595716,
 author = {Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
 title = {Fair and Balanced?: Bias in Bug-fix Datasets},
 booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
 series = {ESEC/FSE '09},
 year = {2009},
 isbn = {978-1-60558-001-2},
 location = {Amsterdam, The Netherlands},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1595696.1595716},
 doi = {10.1145/1595696.1595716},
 acmid = {1595716},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias},
}

% Defexts: A Curated Dataset of Reproducible Real-World Bugs for Modern JVM Languages
% https://ieeexplore.ieee.org/abstract/document/8802677
@INPROCEEDINGS{8802677,
author={S. {Benton} and A. {Ghanbari} and L. {Zhang}},
booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
title={Defexts: A Curated Dataset of Reproducible Real-World Bugs for Modern JVM Languages},
year={2019},
volume={},
number={},
pages={47-50},
keywords={Java;program debugging;programming languages;software engineering;virtual machines;curated dataset;reproducible real-world bugs;software engineering studies;bug detection;benchmark bug datasets;software engineering research;Defexts Kotlin;Defexts Groovy;225 Kotlin;JVM languages;Java virtual machine;JVM programming languages;reproducible bug datasets;dataset;bug;JVM;java virtual machine;software testing;benchmark;kotlin;groovy;scala},
doi={10.1109/ICSE-Companion.2019.00035},
ISSN={},
month={May},
}

% Dissection of a Bug Dataset: Anatomy of 395 Patches from Defects4J
% https://arxiv.org/abs/1801.06393
@article{DBLP:journals/corr/abs-1801-06393,
  author    = {Victor Sobreira and
               Thomas Durieux and
               Fernanda Madeiral Delfim and
               Martin Monperrus and
               Marcelo de Almeida Maia},
  title     = {Dissection of a Bug Dataset: Anatomy of 395 Patches from Defects4J},
  journal   = {CoRR},
  volume    = {abs/1801.06393},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.06393},
  archivePrefix = {arXiv},
  eprint    = {1801.06393},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-06393},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




% How Often Do Single-Statement Bugs Occur?  The ManySStuBs4J Dataset
% https://arxiv.org/abs/1905.13334
@article{DBLP:journals/corr/abs-1905-13334,
  author    = {Rafael{-}Michael Karampatsis and
               Charles Sutton},
  title     = {How Often Do Single-Statement Bugs Occur? The ManySStuBs4J Dataset},
  journal   = {CoRR},
  volume    = {abs/1905.13334},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.13334},
  archivePrefix = {arXiv},
  eprint    = {1905.13334},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-13334},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Qlose: Program Repair with Quantitative Objectives
% (New Rishabh repair paper)
% https://rishabhmit.bitbucket.io/papers/cav16.pdf
@inproceedings{DAntoni2016QlosePR,
  title={Qlose: Program Repair with Quantitative Objectives},
  author={Loris D'Antoni and Roopsha Samanta and Rishabh Singh},
  booktitle={CAV},
  year={2016}
}

% Mining source code repositories at massive scale using language modeling
% https://dl.acm.org/citation.cfm?id=2487085.2487127
@inproceedings{Allamanis:2013:MSC:2487085.2487127,
 author = {Allamanis, Miltiadis and Sutton, Charles},
 title = {Mining Source Code Repositories at Massive Scale Using Language Modeling},
 booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
 series = {MSR '13},
 year = {2013},
 isbn = {978-1-4673-2936-1},
 location = {San Francisco, CA, USA},
 pages = {207--216},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=2487085.2487127},
 acmid = {2487127},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

% Probabilistic model for code with decision trees
% https://dl.acm.org/citation.cfm?doid=2983990.2984041
@inproceedings{Raychev:2016:PMC:2983990.2984041,
 author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
 title = {Probabilistic Model for Code with Decision Trees},
 booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
 series = {OOPSLA 2016},
 year = {2016},
 isbn = {978-1-4503-4444-9},
 location = {Amsterdam, Netherlands},
 pages = {731--747},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/2983990.2984041},
 doi = {10.1145/2983990.2984041},
 acmid = {2984041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Code Completion, Decision Trees, Probabilistic Models of Code},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}

% Linear Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Neural Turing Machines
% https://arxiv.org/abs/1410.5401
% Contains a memory bank that is addressed with soft attention. Tries to learn
% "programs" given inputs and outputs
@article{DBLP:journals/corr/GravesWD14,
  author    = {Alex Graves and
               Greg Wayne and
               Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  archivePrefix = {arXiv},
  eprint    = {1410.5401},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GravesWD14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Code completion with statistical language models
% https://dl.acm.org/citation.cfm?doid=2594291.2594321
% Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.
@inproceedings{Raychev:2014:CCS:2594291.2594321,
 author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
 title = {Code Completion with Statistical Language Models},
 booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI '14},
 year = {2014},
 isbn = {978-1-4503-2784-8},
 location = {Edinburgh, United Kingdom},
 pages = {419--428},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2594291.2594321},
 doi = {10.1145/2594291.2594321},
 acmid = {2594321},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% Transformer:  Attention is all you need
% https://arxiv.org/pdf/1706.03762.pdf
% https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


% BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
% https://arxiv.org/abs/1810.04805
% https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-04805},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

Multi-Task Deep Neural Networks for Natural Language Understanding
https://arxiv.org/abs/1901.11504
% https://venturebeat.com/2019/05/16/microsoft-makes-googles-bert-nlp-model-better/
% https://www.microsoft.com/en-us/research/blog/robust-language-representation-learning-via-multi-task-knowledge-distillation/
@article{DBLP:journals/corr/abs-1901-11504,
  author    = {Xiaodong Liu and
               Pengcheng He and
               Weizhu Chen and
               Jianfeng Gao},
  title     = {Multi-Task Deep Neural Networks for Natural Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1901.11504},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11504},
  archivePrefix = {arXiv},
  eprint    = {1901.11504},
  timestamp = {Mon, 04 Feb 2019 08:11:03 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-11504},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Bidirectional Recurrent Neural Networks as Generative Models - Reconstructing Gaps in Time Series
% https://papers.nips.cc/paper/5651-bidirectional-recurrent-neural-networks-as-generative-models.pdf
% https://arxiv.org/abs/1504.01575
% This is the Gibb's sampling paper
@article{DBLP:journals/corr/BerglundRHKVK15,
  author    = {Mathias Berglund and
               Tapani Raiko and
               Mikko Honkala and
               Leo K{\"{a}}rkk{\"{a}}inen and
               Akos Vetek and
               Juha Karhunen},
  title     = {Bidirectional Recurrent Neural Networks as Generative Models - Reconstructing
               Gaps in Time Series},
  journal   = {CoRR},
  volume    = {abs/1504.01575},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.01575},
  archivePrefix = {arXiv},
  eprint    = {1504.01575},
  timestamp = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BerglundRHKVK15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Bidirectional models: Schuster and Paliwal,
1997
% LSTM: (Hochreiter and Schmidhuber, 1997

% CAPACITY AND TRAINABILITY IN RECURRENT
NEURAL NETWORKS
% https://arxiv.org/pdf/1611.09913.pdf
% TODO: change arxiv to iclr?
% Describes the main advantage of gated units over vanilla RNNs is just trainability.
@article{collins2016capacity,
  title={Capacity and trainability in recurrent neural networks},
  author={Collins, Jasmine and Sohl-Dickstein, Jascha and Sussillo, David},
  journal={arXiv preprint arXiv:1611.09913},
  year={2016}
}

Recurrent networks for structured data - A unifying approach and its properties
% https://dl.acm.org/citation.cfm?id=2298819
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.6673&rep=rep1&type=pdf
% Describes unconstrained decoding as fundamentally, provably harder than encoding.
@article{Hammer:2002:RNS:2298655.2298819,
 author = {Hammer, Barbara},
 title = {Recurrent Networks for Structured Data - A Unifying Approach and Its Properties},
 journal = {Cogn. Syst. Res.},
 issue_date = {June, 2002},
 volume = {3},
 number = {2},
 month = jun,
 year = {2002},
 issn = {1389-0417},
 pages = {145--165},
 numpages = {21},
 url = {http://dx.doi.org/10.1016/S1389-0417(01)00056-0},
 doi = {10.1016/S1389-0417(01)00056-0},
 acmid = {2298819},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Approximation ability, Holographic reduced representation, Hybrid systems, Learnability, Recurrent networks, Recursive autoassociative memory},
}





% A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks (2007)
% http://citeseerx.ist.psu.edu/viewdoc/citations;jsessionid=1F9BF20320553DCCF172315FD28CD453?doi=10.1.1.139.5852
% https://www.semanticscholar.org/paper/A-novel-approach-to-on-line-handwriting-recognition-Liwicki-Graves/bb2795e9dd38a2d324d8bd2042dd9202c6c89726
@inproceedings{Liwicki2007ANA,
  title={A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks},
  author={Marcus Liwicki and Alex Graves and Horst Bunke and Juergen Schmidhuber},
  year={2007}
}

% Asynchronous Bidirectional Decoding for Neural Machine Translation
% https://arxiv.org/pdf/1801.05122.pdf
% This is the bidirectional *decoder* model, which first generates backwards and then forwards the target sentence in a
% language translation task. Only sequential, though.
@article{DBLP:journals/corr/abs-1801-05122,
  author    = {Xiangwen Zhang and
               Jinsong Su and
               Yue Qin and
               Yang Liu and
               Rongrong Ji and
               Hongji Wang},
  title     = {Asynchronous Bidirectional Decoding for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1801.05122},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.05122},
  archivePrefix = {arXiv},
  eprint    = {1801.05122},
  timestamp = {Mon, 15 Jul 2019 14:17:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-05122},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition
%https://arxiv.org/abs/1607.07043
% Stepping stone to tree approach, since now we're incorporating more than one
% sequence into the LSTM. Almost like TreeLSTM? Not sure...
@article{DBLP:journals/corr/LiuSXW16,
  author    = {Jun Liu and
               Amir Shahroudy and
               Dong Xu and
               Gang Wang},
  title     = {Spatio-Temporal {LSTM} with Trust Gates for 3D Human Action Recognition},
  journal   = {CoRR},
  volume    = {abs/1607.07043},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.07043},
  archivePrefix = {arXiv},
  eprint    = {1607.07043},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LiuSXW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}

% Tree Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Program Language Translation Using a Grammar-Driven Tree-to-Tree Model
% https://arxiv.org/abs/1807.01784
% Left-child right-sibling binary trees + TreeLSTM
% Restricts the parametrization of possible child nodes based on the grammar.
% For example, a For can't be a child of a Binop
% That way, don't need to predict end of tree--but what about arbitrarily long
% sequences (Compound)?
% Translated between an iterative and lambda language, but they are both toy /
% simulated languages. They include dataset, though:
% https://www.dropbox.com/sh/1q4aejr57jk40fs/AADKTvgKqLHuIIzNdlANjGRea?dl=0
@article{DBLP:journals/corr/abs-1807-01784,
  author    = {Mehdi Drissi and
               Olivia Watkins and
               Aditya Khant and
               Vivaswat Ojha and
               Pedro Sandoval Segura and
               Rakia Segev and
               Eric Weiner and
               Robert Keller},
  title     = {Program Language Translation Using a Grammar-Driven Tree-to-Tree Model},
  journal   = {CoRR},
  volume    = {abs/1807.01784},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.01784},
  archivePrefix = {arXiv},
  eprint    = {1807.01784},
  timestamp = {Tue, 11 Jun 2019 09:38:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-01784},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% A Syntactic Neural Model for General-Purpose Code Generation
% https://arxiv.org/abs/1704.01696
% Translate natural language descriptions to Python, IFTTT, etc.
% Uses the Python grammar directly, and an action sequence for generating the
% tree by either applying a pre-defined rule or generating a variable token
% This is a bit of a linearization. Also, how do they recognize topology /
% arbitrarily long sequences? At least, gentoken outputs </n> when it no longer
% generates tokens.
% Darn it, this uses Pointer Networks for identifiers...
% Natural language  is  encoded using just (bidirectional) sequence

% TODO: Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al.,
2016)
Code Generation and Analysis Most existing
works on code generation focus on generating
code for domain specific languages (DSLs) (Kushman and Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Parisotto et al., 2016;
Balog et al., 2016). For general-purpose code generation, besides the general framework of Ling
et al. (2016), existing methods often use language
and task-specific rules and strategies (Lei et al.,
2013; Raghothaman et al., 2016). A similar line
is to use NL queries for code retrieval (Wei et al.,
2015; Allamanis et al., 2015). The reverse task of
generating NL summaries from source code has
also been explored (Oda et al., 2015; Iyer et al.,
2016). Finally, there are probabilistic models of
source code (Maddison and Tarlow, 2014; Nguyen
et al., 2013). The most relevant work is Allamanis et al. (2015), which uses a factorized model
to measure semantic relatedness between NL and
ASTs for code retrieval, while our model tackles
the more challenging generation task.
@article{DBLP:journals/corr/YinN17,
  author    = {Pengcheng Yin and
               Graham Neubig},
  title     = {A Syntactic Neural Model for General-Purpose Code Generation},
  journal   = {CoRR},
  volume    = {abs/1704.01696},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.01696},
  archivePrefix = {arXiv},
  eprint    = {1704.01696},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YinN17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Improve Neural Machine Translation by Syntax Tree
% https://dl.acm.org/citation.cfm?id=3284744
% Restricted to a very specific form of binary tree (see figure 2), but does
% propogate information first bottom-up and then top-down
% TODO: In 2017, Kokkinos and Potamianos [8] proposed a bidirectional
tree GRU and applied it to the sentiment analysis task, greatly
improving the performance of the model. (Top-down encoder/decoder?)
@inproceedings{Chen:2018:INM:3284557.3284744,
 author = {Chen, Siyu and Yu, Qingsong},
 title = {Improve Neural Machine Translation by Syntax Tree},
 booktitle = {Proceedings of the 2Nd International Symposium on Computer Science and Intelligent Control},
 series = {ISCSIC '18},
 year = {2018},
 isbn = {978-1-4503-6628-1},
 location = {Stockholm, Sweden},
 pages = {53:1--53:6},
 articleno = {53},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3284557.3284744},
 doi = {10.1145/3284557.3284744},
 acmid = {3284744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {End to end, Machine translation, Neural network, Syntax tree},
}

% Recurrent Neural Network Grammars
% https://arxiv.org/abs/1602.07776
% RNNGs
% Learns a shift-reduce parser and its grammar rules, rather than
% generating trees directly
% Constituency trees
% Has to use importance sampling / Monte Carlo to actually generate words
@article{DBLP:journals/corr/DyerKBS16,
  author    = {Chris Dyer and
               Adhiguna Kuncoro and
               Miguel Ballesteros and
               Noah A. Smith},
  title     = {Recurrent Neural Network Grammars},
  journal   = {CoRR},
  volume    = {abs/1602.07776},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.07776},
  archivePrefix = {arXiv},
  eprint    = {1602.07776},
  timestamp = {Mon, 13 Aug 2018 16:47:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DyerKBS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Learning to Parse and Translate Improves Neural Machine Translation
% https://arxiv.org/abs/1702.03525
% https://github.com/tempra28/nmtrnng
% NMT+RNNG
% Dependency Trees
% Replaces the Monte Carlo in RNNG with just another RNN
% TODO: Aharoni and Goldberg (2017) introduced
a method to serialize a parsed tree and to train the
serialized parsed sentences
@article{DBLP:journals/corr/EriguchiTC17,
  author    = {Akiko Eriguchi and
               Yoshimasa Tsuruoka and
               Kyunghyun Cho},
  title     = {Learning to Parse and Translate Improves Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1702.03525},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03525},
  archivePrefix = {arXiv},
  eprint    = {1702.03525},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/EriguchiTC17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





% A novel neural source code representation based on abstract syntax tree
% https://dl.acm.org/citation.cfm?id=3339604
% ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment
@inproceedings{Zhang:2019:NNS:3339505.3339604,
 author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
 title = {A Novel Neural Source Code Representation Based on Abstract Syntax Tree},
 booktitle = {Proceedings of the 41st International Conference on Software Engineering},
 series = {ICSE '19},
 year = {2019},
 location = {Montreal, Quebec, Canada},
 pages = {783--794},
 numpages = {12},
 url = {https://doi.org/10.1109/ICSE.2019.00086},
 doi = {10.1109/ICSE.2019.00086},
 acmid = {3339604},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {abstract syntax tree, code classification, code clone detection, neural network, source code representation},
}


% code2vec: Learning Distributed Representations of Code
% https://arxiv.org/abs/1803.09473
% The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path simultaneously with learning how to aggregate a set of them
@article{DBLP:journals/corr/abs-1803-09473,
  author    = {Uri Alon and
               Meital Zilberstein and
               Omer Levy and
               Eran Yahav},
  title     = {code2vec: Learning Distributed Representations of Code},
  journal   = {CoRR},
  volume    = {abs/1803.09473},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09473},
  archivePrefix = {arXiv},
  eprint    = {1803.09473},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-09473},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Tree­based Translation without Using Parse Trees
% https://www.aclweb.org/anthology/C12-1186
% https://www.researchgate.net/publication/270877912_Tree-based_Translation_without_using_Parse_Trees
% Uses unsupervised methods for getting tree represenntations of languages where
% we can't readily parse (e.g., not enough data to have a full parser)
@inproceedings{inproceedings,
author = {Zhai, Feifei and Zhang, Jiajun and Zhou, Yu and Zong, Chengqing},
year = {2012},
month = {12},
pages = {3037-3054},
title = {Tree-based Translation without using Parse Trees},
journal = {24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers}
}


% A general path-based representation for predicting program properties
% https://dl.acm.org/citation.cfm?doid=3192366.3192412
% Uses the AST
@inproceedings{Alon:2018:GPR:3192366.3192412,
 author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
 title = {A General Path-based Representation for Predicting Program Properties},
 booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2018},
 year = {2018},
 isbn = {978-1-4503-5698-5},
 location = {Philadelphia, PA, USA},
 pages = {404--419},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3192366.3192412},
 doi = {10.1145/3192366.3192412},
 acmid = {3192412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Big Code, Learning Representations, Machine Learning, Programming Languages},
}


Tree-structured decoding with doubly-recurrent neural networks
% https://openreview.net/pdf?id=HkYhZDqxg
% DRNN Paper!
% neural networks (RNNs) are the best known and
most widely used neural network (NN) model for sequence
data as they sequentially scan the entire sequence and generate
a compressed form of it. Although in theory RNNs are capable
of remembering long distance dependencies, practically, as the
sequence be
@article{alvarez2016tree,
  title={Tree-structured decoding with doubly-recurrent neural networks},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  journal={ICLR},
  year={2016}
}

% Improving Tree-LSTM with Tree Attention
% https://arxiv.org/abs/1901.00066
@article{DBLP:journals/corr/abs-1901-00066,
  author    = {Mahtab Ahmed and
               Muhammad Rifayat Samee and
               Robert E. Mercer},
  title     = {Improving Tree-LSTM with Tree Attention},
  journal   = {CoRR},
  volume    = {abs/1901.00066},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.00066},
  archivePrefix = {arXiv},
  eprint    = {1901.00066},
  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-00066},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks
https://arxiv.org/pdf/1503.00075.pdf
% TreeLSTM
@article{DBLP:journals/corr/TaiSM15,
  author    = {Kai Sheng Tai and
               Richard Socher and
               Christopher D. Manning},
  title     = {Improved Semantic Representations From Tree-Structured Long Short-Term
               Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1503.00075},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.00075},
  archivePrefix = {arXiv},
  eprint    = {1503.00075},
  timestamp = {Mon, 13 Aug 2018 16:48:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TaiSM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Top-down Tree Long Short-Term Memory Networks
% https://arxiv.org/pdf/1511.00060.pdf
% The OTHER TreeLSTM paper, for *decoding*
% Uses the 4 different LSTMs, basically just another example of a possible decomposition. "Hard"-selects LSTM to use.
% LDTreeLSTM is kind of like an alternating bidirectional (left) decoder/ (left) encoder/ (right) decoder
@article{DBLP:journals/corr/ZhangLL15,
  author    = {Xingxing Zhang and
               Liang Lu and
               Mirella Lapata},
  title     = {Tree Recurrent Neural Networks with Application to Language Modeling},
  journal   = {CoRR},
  volume    = {abs/1511.00060},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.00060},
  archivePrefix = {arXiv},
  eprint    = {1511.00060},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhangLL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% When Are Tree Structures Necessary for Deep Learning of
Representations?
% https://nlp.stanford.edu/pubs/emnlp2015_2_jiwei.pdf
@article{DBLP:journals/corr/LiJH15,
  author    = {Jiwei Li and
               Dan Jurafsky and
               Eduard H. Hovy},
  title     = {When Are Tree Structures Necessary for Deep Learning of Representations?},
  journal   = {CoRR},
  volume    = {abs/1503.00185},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.00185},
  archivePrefix = {arXiv},
  eprint    = {1503.00185},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LiJH15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Text Summarization as Tree Transduction by Top-Down TreeLSTM
% https://ieeexplore.ieee.org/abstract/document/8628873
% TD-TreeLSTM: top-down *encoding*. Basically, stuck with  a subset of the tree shape. So we use it to "compress"
% sentences to something with the same meaning. Literally just uses the "parent" relation
@INPROCEEDINGS{8628873,
author={D. {Bacciu} and A. {Bruno}},
booktitle={2018 IEEE Symposium Series on Computational Intelligence (SSCI)},
title={Text Summarization as Tree Transduction by Top-Down TreeLSTM},
year={2018},
volume={},
number={},
pages={1411-1418},
keywords={data compression;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;trees (mathematics);text summarization;neural extractive compression;parse tree transduction problem;sequence transduction task;deep neural model;standard Long Short-Term Memory;structural recursion;sentence compression benchmarks;natural language processing problem;top-down TreeLSTM;Task analysis;Encoding;Kernel;Skeleton;Grammar;Syntactics;Feature extraction;structured-data processing;tree transduction;top-down TreeLSTM;sentence compression},
doi={10.1109/SSCI.2018.8628873},
ISSN={},
month={Nov},
}



% End-to-End Relation Extraction using LSTMs
on Sequences and Tree Structures
% https://www.aclweb.org/anthology/P16-1105
% Handles different "types" of children by using different weights for them in the standard treelstm, rather than handle
% them independently. This is the top-down and bottom-up encoding paper, and also includes sequential encoding
@article{DBLP:journals/corr/MiwaB16,
  author    = {Makoto Miwa and
               Mohit Bansal},
  title     = {End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures},
  journal   = {CoRR},
  volume    = {abs/1601.00770},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.00770},
  archivePrefix = {arXiv},
  eprint    = {1601.00770},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MiwaB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%  Motivated by the success of adding syntactic information to Statistical
Machine Translation (SMT) (Galley et al., 2004;
Menezes and Quirk, 2007; Galley et al., 2006), recent works have established that explicitly leveraging syntactic information can improve NMT
1Our code is available at
https://github.com/cindyxinyiwang/TrDec_pytorch.
quality, either through syntactic encoders (Li et al.,
2017; Eriguchi et al., 2016), multi-task learning objectives (Chen et al., 2017; Eriguchi et al.,
2017), or direct addition of syntactic tokens
to the target sequence (Nadejde et al., 2017;
Aharoni and Goldberg, 2017)
% A Tree-based Decoder for Neural Machine Translation
% https://arxiv.org/pdf/1808.09374.pdf
% TrDec employs two RNNs: a rule RNN, which
tracks the topology of the tree based on rules
defined by a Context Free Grammar (CFG),
and a word RNN, which tracks words at the
leaves of the tree (§ 3).
% The generation process is guided by a
CFG over target trees, which is constructed by taking all production rules extracted from the trees of
all sentences in the training corpus. Specifically, a
rule RNN first generates the top of the tree structure, and continues until a preterminal is reached.
Then, a word RNN fills out the words under the
preterminal. The model switches back to the rule
RNN after the word RNN finishes
% Uses a seq encoder
@article{DBLP:journals/corr/abs-1808-09374,
  author    = {Xinyi Wang and
               Hieu Pham and
               Pengcheng Yin and
               Graham Neubig},
  title     = {A Tree-based Decoder for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1808.09374},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.09374},
  archivePrefix = {arXiv},
  eprint    = {1808.09374},
  timestamp = {Mon, 03 Sep 2018 13:36:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-09374},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Multiplicative Tree-Structured Long Short-Term Memory Networks for Semantic Representations
% https://www.aclweb.org/anthology/S18-2032
% Adds in different edge types into the parameterization
@inproceedings{tran2018multiplicative,
  title={Multiplicative tree-structured long short-term memory networks for semantic representations},
  author={Tran, Nam Khanh and Cheng, Weiwei},
  booktitle={Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics},
  pages={276--286},
  year={2018}
}

% Head-Lexicalized Bidirectional Tree LSTMs
% https://www.aclweb.org/anthology/Q17-1012
% https://arxiv.org/abs/1611.06788
% Bi-directional is just parent-to-child, but with different weights for left vs right child (only binary trees)
@article{DBLP:journals/corr/TengZ16,
  author    = {Zhiyang Teng and
               Yue Zhang},
  title     = {Bidirectional Tree-Structured {LSTM} with Head Lexicalization},
  journal   = {CoRR},
  volume    = {abs/1611.06788},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.06788},
  archivePrefix = {arXiv},
  eprint    = {1611.06788},
  timestamp = {Mon, 13 Aug 2018 16:46:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TengZ16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Generation of code from text description with syntactic parsing and Tree2Tree model
% https://pdfs.semanticscholar.org/28d1/6be7f51a6b53b55958aa024e857a1c4361f2.pdf
% http://www.er.ucu.edu.ua:8080/handle/1/1191
% This paper does code generation with tree2tree and uses pointer networks
@inproceedings{Stehnii2018GenerationOC,
  title={Generation of code from text description with syntactic parsing and Tree2Tree model},
  author={Anatolii Stehnii},
  year={2018}
}

% Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing
% https://arxiv.org/pdf/1809.01854
% Seq2DRNN+SynC, just adds in connenction to the parent's left sibling for a node without a left sibling. Also adds
% sequential leaf word RNN?

% Also adds in attention over encoded states
% TODO: Aharoni and Goldberg (2017), Wu et al. (2017),
and Eriguchi et al. (2017) experimented with NMT
models that utilise target side structural syntax.
Aharoni and Goldberg (2017) treated constituency
trees as sequential strings (linearised-tree) and
trained a Seq2Seq model to produce such sequences. Wu et al. (2017) proposed SD-NMT,
which models dependency syntax trees by adding
a shift-reduce neural parser to a standard RNN
decoder. Eriguchi et al. (2017) in addition to
Wu et al. (2017)’s work, proposed NMT+RNNG
which uses a modified RNNG generator (Dyer
et al., 2016) to process dependency instead of
constituency information as originally proposed
by Dyer et al. (2016), making it consequently
a StackLSTM sequential decoder with additional
RNN units so it is still a bottom-up tree-structured
decoder rather than a top-down decoder like ours.
Nevertheless, all of these research showed that
target side syntax could improve NMT systems.
We believe these models could also be augmented
with SynC connections (with NMT+RNNG one
has to instead use constituency information).
@article{DBLP:journals/corr/abs-1809-01854,
  author    = {Jetic Gu and
               Hassan Shavarani and
               Anoop Sarkar},
  title     = {Top-down Tree Structured Decoding with Syntactic Connections for Neural
               Machine Translation and Parsing},
  journal   = {CoRR},
  volume    = {abs/1809.01854},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.01854},
  archivePrefix = {arXiv},
  eprint    = {1809.01854},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-01854},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Tree-Structured Neural Machine
for Linguistics-Aware Sentence Generation
% https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16567/16133
% https://arxiv.org/pdf/1705.00321.pdf
% X2Tree paper.Tree decoder
% Tree canonicalization. Uses an LCRS and SP tree forms.
%  Overall, hidden states of a tree-structured decoder need
store less information than chain-structured decoder’s. This
makes X2TREE potentially capable to handle more complex
semantic structures in the response utterances.
% Only decodes, does not yet use a tree-structured encoder.
% All previous sibling hidden states are passed to the current node being generated.  Basically, parent and all left
% siblings (residual connections)
@article{DBLP:journals/corr/ZhouLCXLCH17,
  author    = {Ganbin Zhou and
               Ping Luo and
               Rongyu Cao and
               Yijun Xiao and
               Fen Lin and
               Bo Chen and
               Qing He},
  title     = {Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation},
  journal   = {CoRR},
  volume    = {abs/1705.00321},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.00321},
  archivePrefix = {arXiv},
  eprint    = {1705.00321},
  timestamp = {Wed, 10 Oct 2018 10:26:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhouLCXLCH17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Generation of code from text description with syntactic parsing and Tree2Tree model
% https://www.semanticscholar.org/paper/Generation-of-code-from-text-description-with-and-Stehnii/28d16be7f51a6b53b55958aa024e857a1c4361f2
@inproceedings{Stehnii2018GenerationOC,
  title={Generation of code from text description with syntactic parsing and Tree2Tree model},
  author={Anatolii Stehnii},
  year={2018}
}

% Tree2Tree Learning with Memory Unit
% https://openreview.net/forum?id=Syt0r4bRZ
% Rejected Paper
@misc{
miao2018treetree,
title={Tree2Tree Learning with Memory Unit},
author={Ning Miao and Hengliang Wang and Ran Le and Chongyang Tao and Mingyue Shang and Rui Yan and Dongyan Zhao},
year={2018},
url={https://openreview.net/forum?id=Syt0r4bRZ},
}

% Tree-to-tree Neural Networks for Program Translation
% https://arxiv.org/abs/1802.03691
@article{DBLP:journals/corr/abs-1802-03691,
  author    = {Xinyun Chen and
               Chang Liu and
               Dawn Song},
  title     = {Tree-to-tree Neural Networks for Program Translation},
  journal   = {CoRR},
  volume    = {abs/1802.03691},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.03691},
  archivePrefix = {arXiv},
  eprint    = {1802.03691},
  timestamp = {Mon, 22 Jul 2019 13:37:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-03691},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CODIT: Code Editing with Tree-Based NeuralMachine Translation
% (Originally: Tree2Tree Neural Translation Model for Learning Source Code Changes)
% https://arxiv.org/abs/1810.00314
% https://www.researchgate.net/publication/328016234_Tree2Tree_Neural_Translation_Model_for_Learning_Source_Code_Changes
@article{DBLP:journals/corr/abs-1810-00314,
  author    = {Saikat Chakraborty and
               Miltiadis Allamanis and
               Baishakhi Ray},
  title     = {Tree2Tree Neural Translation Model for Learning Source Code Changes},
  journal   = {CoRR},
  volume    = {abs/1810.00314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.00314},
  archivePrefix = {arXiv},
  eprint    = {1810.00314},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-00314},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks
% https://arxiv.org/abs/1810.09536
% https://openreview.net/forum?id=B1l6qiR5F7
@article{DBLP:journals/corr/abs-1810-09536,
  author    = {Yikang Shen and
               Shawn Tan and
               Alessandro Sordoni and
               Aaron C. Courville},
  title     = {Ordered Neurons: Integrating Tree Structures into Recurrent Neural
               Networks},
  journal   = {CoRR},
  volume    = {abs/1810.09536},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.09536},
  archivePrefix = {arXiv},
  eprint    = {1810.09536},
  timestamp = {Wed, 31 Oct 2018 14:24:29 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-09536},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Sequence-to-Dependency Neural Machine Translation
% https://www.aclweb.org/anthology/P17-1065
% https://www.researchgate.net/publication/318740820_Sequence-to-Dependency_Neural_Machine_Translation
@inproceedings{inproceedings,
author = {Wu, Shuangzhi and Zhang, Dongdong and Yang, Nan and Li, Mu and Zhou, Ming},
year = {2017},
month = {01},
pages = {698-707},
title = {Sequence-to-Dependency Neural Machine Translation},
doi = {10.18653/v1/P17-1065}
}

% Dependency-to-Dependency Neural Machine Translation
% https://dl.acm.org/citation.cfm?id=3281242
@article{Wu:2018:DNM:3281228.3281242,
 author = {Wu, Shuangzhi and Zhang, Dongdong and Zhang, Zhirui and Yang, Nan and Li, Mu and Zhou, Ming},
 title = {Dependency-to-Dependency Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {November 2018},
 volume = {26},
 number = {11},
 month = nov,
 year = {2018},
 issn = {2329-9290},
 pages = {2132--2141},
 numpages = {10},
 url = {https://doi.org/10.1109/TASLP.2018.2855968},
 doi = {10.1109/TASLP.2018.2855968},
 acmid = {3281242},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}

% Graph Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Deriving Neural Architectures from Sequence and Graph Kernels
% https://arxiv.org/abs/1705.09037
@article{DBLP:journals/corr/LeiJBJ17,
  author    = {Tao Lei and
               Wengong Jin and
               Regina Barzilay and
               Tommi S. Jaakkola},
  title     = {Deriving Neural Architectures from Sequence and Graph Kernels},
  journal   = {CoRR},
  volume    = {abs/1705.09037},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.09037},
  archivePrefix = {arXiv},
  eprint    = {1705.09037},
  timestamp = {Fri, 19 Jul 2019 09:36:47 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LeiJBJ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Graph Learning Network: A Structure Learning Algorithm
% https://arxiv.org/abs/1905.12665
@article{DBLP:journals/corr/abs-1905-12665,
  author    = {Darwin Saire Pilco and
               Ad{\'{\i}}n Ram{\'{\i}}rez Rivera},
  title     = {Graph Learning Network: {A} Structure Learning Algorithm},
  journal   = {CoRR},
  volume    = {abs/1905.12665},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12665},
  archivePrefix = {arXiv},
  eprint    = {1905.12665},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-12665},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% How Powerful are Graph Neural Networks?
% https://openreview.net/forum?id=ryGs6iA5Km
% https://arxiv.org/abs/1810.00826
@article{DBLP:journals/corr/abs-1810-00826,
  author    = {Keyulu Xu and
               Weihua Hu and
               Jure Leskovec and
               Stefanie Jegelka},
  title     = {How Powerful are Graph Neural Networks?},
  journal   = {CoRR},
  volume    = {abs/1810.00826},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.00826},
  archivePrefix = {arXiv},
  eprint    = {1810.00826},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-00826},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Computational Capabilities of Graph Neural Networks
% https://dl.acm.org/citation.cfm?id=1657483
% https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4703190&tag=1
@article{Scarselli:2009:CCG:1657477.1657483,
 author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
 title = {Computational Capabilities of Graph Neural Networks},
 journal = {Trans. Neur. Netw.},
 issue_date = {January 2009},
 volume = {20},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1045-9227},
 pages = {81--102},
 numpages = {22},
 url = {http://dx.doi.org/10.1109/TNN.2008.2005141},
 doi = {10.1109/TNN.2008.2005141},
 acmid = {1657483},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Approximation theory, approximation theory, graph neural networks (GNNs), graphical domains, universal approximators},
}

% Neural Message Passing for Quantum Chemistry
% https://arxiv.org/pdf/1704.01212
% Message passing neural network paper (MPNN)
% Generalizes Gated Graph Neural Networks (GG-NN), Li et al.  (2016)
@article{DBLP:journals/corr/GilmerSRVD17,
  author    = {Justin Gilmer and
               Samuel S. Schoenholz and
               Patrick F. Riley and
               Oriol Vinyals and
               George E. Dahl},
  title     = {Neural Message Passing for Quantum Chemistry},
  journal   = {CoRR},
  volume    = {abs/1704.01212},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.01212},
  archivePrefix = {arXiv},
  eprint    = {1704.01212},
  timestamp = {Mon, 13 Aug 2018 16:48:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GilmerSRVD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Embeddings and Representation Learning for Structured Data
% https://arxiv.org/abs/1905.06147
@article{DBLP:journals/corr/abs-1905-06147,
  author    = {Benjamin Paa{\ss}en and
               Claudio Gallicchio and
               Alessio Micheli and
               Alessandro Sperduti},
  title     = {Embeddings and Representation Learning for Structured Data},
  journal   = {CoRR},
  volume    = {abs/1905.06147},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.06147},
  archivePrefix = {arXiv},
  eprint    = {1905.06147},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-06147},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% A Comprehensive Survey on Graph Neural Networks
% https://arxiv.org/pdf/1901.00596.pdf
@article{DBLP:journals/corr/abs-1901-00596,
  author    = {Zonghan Wu and
               Shirui Pan and
               Fengwen Chen and
               Guodong Long and
               Chengqi Zhang and
               Philip S. Yu},
  title     = {A Comprehensive Survey on Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1901.00596},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.00596},
  archivePrefix = {arXiv},
  eprint    = {1901.00596},
  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-00596},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% General Graphs
% LEARNING TO REPRESENT PROGRAMS WITH GRAPHS
% https://arxiv.org/pdf/1711.00740.pdf
@article{DBLP:journals/corr/abs-1711-00740,
  author    = {Miltiadis Allamanis and
               Marc Brockschmidt and
               Mahmoud Khademi},
  title     = {Learning to Represent Programs with Graphs},
  journal   = {CoRR},
  volume    = {abs/1711.00740},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00740},
  archivePrefix = {arXiv},
  eprint    = {1711.00740},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00740},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% GraphSeq2Seq: Graph-Sequence-to-Sequence for Neural Machine Translation
% https://openreview.net/forum?id=B1fA3oActQ
% (Rejected)
@misc{
zhao2019graphseqseq,
title={GraphSeq2Seq: Graph-Sequence-to-Sequence for Neural Machine Translation},
author={Guoshuai Zhao and Jun Li and Lu Wang and Xueming Qian and Yun Fu},
year={2019},
url={https://openreview.net/forum?id=B1fA3oActQ},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}

% Other Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{1

% Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination
% https://arxiv.org/abs/1805.01978
% Treats each instance as basically its own class, and ultimately doesn't use a
% NN (falls back on Monte Carlo Simulation)
@article{DBLP:journals/corr/abs-1805-01978,
  author    = {Zhirong Wu and
               Yuanjun Xiong and
               Stella Yu and
               Dahua Lin},
  title     = {Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
  journal   = {CoRR},
  volume    = {abs/1805.01978},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.01978},
  archivePrefix = {arXiv},
  eprint    = {1805.01978},
  timestamp = {Mon, 13 Aug 2018 16:47:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-01978},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Distilling the Knowledge in a Neural Network
% https://arxiv.org/abs/1503.02531
% Make small neural networks by training them to match the output of a larger
% one, and then you can make an ensemble of those small  ones
@ARTICLE{2015arXiv150302531H,
       author = {{Hinton}, Geoffrey and {Vinyals}, Oriol and {Dean}, Jeff},
        title = "{Distilling the Knowledge in a Neural Network}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2015",
        month = "Mar",
          eid = {arXiv:1503.02531},
        pages = {arXiv:1503.02531},
archivePrefix = {arXiv},
       eprint = {1503.02531},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150302531H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning
% https://arxiv.org/abs/1808.06740
% Doesn't use Deep Learning, instead Markov Decision Processes / Reinforcement
% learning
@article{DBLP:journals/corr/abs-1808-06740,
  author    = {Ziyu Yao and
               Xiujun Li and
               Jianfeng Gao and
               Brian M. Sadler and
               Huan Sun},
  title     = {Interactive Semantic Parsing for If-Then Recipes via Hierarchical
               Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1808.06740},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.06740},
  archivePrefix = {arXiv},
  eprint    = {1808.06740},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-06740},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%1}}}
